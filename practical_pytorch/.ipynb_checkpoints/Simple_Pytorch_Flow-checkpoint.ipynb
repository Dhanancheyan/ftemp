{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()])) \n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use DataLoader for feeding data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 1, 4, 1, 8, 2, 4, 6, 2, 9])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe5d0cf0898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data is Balanced or Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949} 60000\n"
     ]
    }
   ],
   "source": [
    "count_total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for x,y in trainset:\n",
    "    for i in y:\n",
    "        counter_dict[int(i)] += 1\n",
    "        count_total += 1\n",
    "        \n",
    "print(counter_dict, count_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for label in counter_dict:\n",
    "    print(f\"{label}: {counter_dict[label]/count_total * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3715, -2.3256, -2.3813, -2.2887, -2.2217, -2.3963, -2.1602, -2.4073,\n",
      "         -2.3028, -2.2045]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "xi = torch.rand(28,28)\n",
    "xi = xi.reshape(1,28*28)\n",
    "print(net(xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_fn, optimizer, train_dl, valid_dl, n_epochs: int):\n",
    "    for epoch in range(n_epochs):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        for batch_idx, (x, y) in enumerate(train_dl):\n",
    "            # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "            x = x.view(-1, 28*28)\n",
    "#             x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            y_ = model(x)\n",
    "            loss = loss_fn(y_, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            if batch_idx % 1000 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(x), len(train_dl.dataset),\n",
    "                               100. * batch_idx / len(train_dl), loss.data))\n",
    "            \n",
    "        # EVAL\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_dl:\n",
    "                x = x.view(-1, 28*28)\n",
    "#                 x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                y_ = model(x)\n",
    "                loss = loss_fn(y_, y)\n",
    "                # sum up batch loss\n",
    "                test_loss += loss_fn(y_, y).data\n",
    "                pred = y_.data.max(1)[1]  # get the index of the max log-probability\n",
    "                correct += pred.eq(y.data).sum()\n",
    "    \n",
    "        # Statistics  \n",
    "        test_loss /= len(valid_dl.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(valid_dl.dataset),\n",
    "                100. * correct / len(valid_dl.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.262376\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.585907\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.379911\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.432950\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.884550\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.205294\n",
      "\n",
      "Test set: Average loss: 0.0423, Accuracy: 8758/10000 (87%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.743582\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.326777\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.059305\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.735474\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.716863\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.294879\n",
      "\n",
      "Test set: Average loss: 0.0385, Accuracy: 8870/10000 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(net, criterion, optimizer, trainset, testset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the main training loop\n",
    "net.train()\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(trainset):\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(trainset.dataset),\n",
    "                           100. * batch_idx / len(trainset), loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a test loop\n",
    "net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in testset:\n",
    "        data = data.view(-1, 28*28)\n",
    "        net_out = net(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).data\n",
    "        pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(testset.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testset.dataset),\n",
    "        100. * correct / len(testset.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
